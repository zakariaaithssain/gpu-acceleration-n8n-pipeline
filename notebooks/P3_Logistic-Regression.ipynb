{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark d'Accélération GPU vs CPU\n",
    "## Régression Logistique : quand le CPU bat le GPU\n",
    "\n",
    "---\n",
    "\n",
    "### Objectifs\n",
    "\n",
    "Ce notebook démontre que **l'utilisation d'un GPU n'est pas toujours avantageuse** : pour des modèles simples comme la régression logistique, le CPU peut être plus rapide que le GPU.\n",
    "\n",
    "**Points clés :**\n",
    "- Mesure précise du temps d'exécution CPU vs GPU sur un modèle de régression logistique\n",
    "- Utilisation de `torch.profiler` pour analyser les performances\n",
    "- Calcul du **Speed Up** (rapport de performance)\n",
    "- **Conclusion :** Overhead de transfert CPU↔GPU et faible parallélisme → CPU souvent gagnant\n",
    "\n",
    "---\n",
    "\n",
    "### Contexte : Modèles simples vs modèles complexes\n",
    "\n",
    "Les GPU excellent sur les opérations massivement parallèles (réseaux profonds, gros batchs). Pour un modèle linéaire avec peu de paramètres et des données de taille modérée, le coût de transfert des données vers le GPU et le lancement des kernels CUDA peut dépasser le gain de calcul."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Installation des Dépendances\n",
    "\n",
    "Installez les bibliothèques nécessaires si ce n'est pas déjà fait :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cpu)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.3)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.10.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (26.0)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (12.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch numpy matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Imports et Vérification GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      " VÉRIFICATION DE L'ENVIRONNEMENT\n",
      "======================================================================\n",
      "PyTorch version : 2.9.0+cpu\n",
      "CUDA disponible : False\n",
      "  Aucun GPU détecté - Le benchmark ne comparera que le CPU\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import statistics\n",
    "import numpy as np\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "# Vérification de la disponibilité du GPU\n",
    "print(\"=\"*70)\n",
    "print(\" VÉRIFICATION DE L'ENVIRONNEMENT\")\n",
    "print(\"=\"*70)\n",
    "print(f\"PyTorch version : {torch.__version__}\")\n",
    "print(f\"CUDA disponible : {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU détecté : {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Mémoire GPU : {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "    print(f\"CUDA version : {torch.version.cuda}\")\n",
    "else:\n",
    "    print(\"  Aucun GPU détecté - Le benchmark ne comparera que le CPU\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Configuration du Benchmark\n",
    "\n",
    "### Paramètres importants :\n",
    "\n",
    "- **NUM_SAMPLES** : Nombre d'échantillons (taille du dataset)\n",
    "- **NUM_FEATURES** : Nombre de features (dimension d'entrée)\n",
    "- **BATCH_SIZE** : Taille des batchs pour l'inférence\n",
    "- **NUM_RUNS** : Nombre d'exécutions pour statistiques robustes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Configuration du benchmark :\n",
      "   • Nombre d'échantillons : 10000\n",
      "   • Nombre de features : 100\n",
      "   • Batch size CPU : 256\n",
      "   • Batch size GPU : 512\n",
      "   • Runs par device : 3\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "NUM_SAMPLES = 10000\n",
    "NUM_FEATURES = 100\n",
    "NUM_RUNS = 3\n",
    "BATCH_SIZE_CPU = 256\n",
    "BATCH_SIZE_GPU = 512\n",
    "\n",
    "print(f\" Configuration du benchmark :\")\n",
    "print(f\"   • Nombre d'échantillons : {NUM_SAMPLES}\")\n",
    "print(f\"   • Nombre de features : {NUM_FEATURES}\")\n",
    "print(f\"   • Batch size CPU : {BATCH_SIZE_CPU}\")\n",
    "print(f\"   • Batch size GPU : {BATCH_SIZE_GPU}\")\n",
    "print(f\"   • Runs par device : {NUM_RUNS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Préparation des Données\n",
    "\n",
    "Génération de données synthétiques pour un problème de classification binaire (régression logistique)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Dataset créé : 10000 échantillons, 100 features\n",
      " Répartition des classes : [5007 4993]\n",
      "\n",
      "Exemple (5 premières lignes, 3 features) :\n",
      "[[ 0.49671414 -0.1382643   0.64768857]\n",
      " [-1.4153707  -0.42064533 -0.34271452]\n",
      " [ 0.35778737  0.5607845   1.0830512 ]\n",
      " [-0.828995   -0.560181    0.7472936 ]\n",
      " [-1.5944277  -0.599375    0.0052437 ]]\n"
     ]
    }
   ],
   "source": [
    "# Données synthétiques : classification binaire\n",
    "np.random.seed(42)\n",
    "X_np = np.random.randn(NUM_SAMPLES, NUM_FEATURES).astype(np.float32)\n",
    "y_np = (X_np[:, 0] + 0.5 * X_np[:, 1] + np.random.randn(NUM_SAMPLES) * 0.3 > 0).astype(np.int64)\n",
    "\n",
    "print(f\" Dataset créé : {X_np.shape[0]} échantillons, {X_np.shape[1]} features\")\n",
    "print(f\" Répartition des classes : {np.bincount(y_np)}\")\n",
    "print(f\"\\nExemple (5 premières lignes, 3 features) :\")\n",
    "print(X_np[:5, :3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Définition du Modèle (Régression Logistique)\n",
    "\n",
    "Régression logistique = une couche linéaire + sigmoid. Modèle très léger (peu de paramètres), peu adapté au parallélisme massif du GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Modèle : LogisticRegression (PyTorch)\n",
      " Nombre de paramètres : 101\n",
      " Modèle chargé avec succès.\n"
     ]
    }
   ],
   "source": [
    "class LogisticRegressionPyTorch(nn.Module):\n",
    "    \"\"\"Régression logistique : Linear + Sigmoid.\"\"\"\n",
    "    def __init__(self, in_features, num_classes=1):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.linear(x)).squeeze(-1)\n",
    "\n",
    "\n",
    "model = LogisticRegressionPyTorch(NUM_FEATURES)\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\" Modèle : LogisticRegression (PyTorch)\")\n",
    "print(f\" Nombre de paramètres : {num_params}\")\n",
    "print(f\" Modèle chargé avec succès.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Fonction de Benchmark avec Profilage\n",
    "\n",
    "### Points clés :\n",
    "\n",
    "1. **Warm-up** : Le premier passage sur GPU initialise les kernels CUDA (overhead)\n",
    "2. **torch.profiler** : Capture les métriques CPU et CUDA\n",
    "3. **record_function** : Isole l'opération critique\n",
    "4. **Export de traces** : Pour visualisation dans Chrome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_benchmark(device_name, batch_size, X_tensor, run_number=1):\n",
    "    \"\"\"\n",
    "    Exécute un benchmark d'inférence sur le device spécifié avec profilage.\n",
    "\n",
    "    Args:\n",
    "        device_name: 'cpu' ou 'cuda'\n",
    "        batch_size: Taille du batch pour l'inférence\n",
    "        X_tensor: Données déjà sur le bon device\n",
    "        run_number: Numéro de l'exécution (pour affichage)\n",
    "\n",
    "    Returns:\n",
    "        float: Temps d'exécution en secondes\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'─' * 70}\")\n",
    "    print(f\" RUN #{run_number} - Device : {device_name.upper()} (Batch size: {batch_size})\")\n",
    "    print(f\"{'─' * 70}\")\n",
    "\n",
    "    device = torch.device(device_name)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # WARM-UP\n",
    "    if run_number == 1:\n",
    "        print(\"   Warm-up (chauffe du GPU/CPU)...\")\n",
    "        with torch.no_grad():\n",
    "            _ = model(X_tensor[:batch_size])\n",
    "        print(\"   Warm-up terminé\")\n",
    "\n",
    "    activities = [ProfilerActivity.CPU]\n",
    "    if device_name == 'cuda':\n",
    "        activities.append(ProfilerActivity.CUDA)\n",
    "\n",
    "    print(\"   Démarrage du profilage...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        with profile(\n",
    "            activities=activities,\n",
    "            record_shapes=True,\n",
    "            profile_memory=True,\n",
    "            with_stack=False\n",
    "        ) as prof:\n",
    "            with record_function(\"model_inference\"):\n",
    "                for i in range(0, len(X_tensor), batch_size):\n",
    "                    batch = X_tensor[i:i + batch_size]\n",
    "                    _ = model(batch)\n",
    "\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "\n",
    "    print(f\"\\n   Temps d'exécution : {total_time:.4f} secondes\")\n",
    "\n",
    "    print(f\"\\n   Analyse du profiler ({device_name.upper()}):\")\n",
    "    for evt in prof.key_averages():\n",
    "        if evt.key == \"model_inference\":\n",
    "            print(f\"     • Temps CPU total : {evt.cpu_time_total / 1000:.2f} ms\")\n",
    "            if device_name == 'cuda' and hasattr(evt, 'self_cuda_time_total'):\n",
    "                print(f\"     • Temps CUDA total : {evt.self_cuda_time_total / 1000:.2f} ms\")\n",
    "\n",
    "    if run_number == 1:\n",
    "        print(f\"\\n   Top 5 des opérations ({device_name.upper()}):\")\n",
    "        sort_key = \"self_cuda_time_total\" if device_name == 'cuda' else \"cpu_time_total\"\n",
    "        print(prof.key_averages().table(sort_by=sort_key, row_limit=5))\n",
    "        trace_file = f\"trace_lr_{device_name}_run{run_number}.json\"\n",
    "        prof.export_chrome_trace(trace_file)\n",
    "        print(f\"\\n   Trace exportée : {trace_file}\")\n",
    "        print(f\"     → Visualiser dans Chrome : chrome://tracing\")\n",
    "\n",
    "    return total_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 Exécution du Benchmark CPU\n",
    "\n",
    "### Pourquoi 3 runs ?\n",
    "- Éliminer les variations dues au système d'exploitation\n",
    "- Calculer des statistiques fiables (médiane, écart-type)\n",
    "- Détecter les outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "  PHASE 1 : BENCHMARK CPU\n",
      "======================================================================\n",
      "\n",
      "──────────────────────────────────────────────────────────────────────\n",
      " RUN #1 - Device : CPU (Batch size: 256)\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "   Warm-up (chauffe du GPU/CPU)...\n",
      "   Warm-up terminé\n",
      "   Démarrage du profilage...\n",
      "\n",
      "   Temps d'exécution : 3.0640 secondes\n",
      "\n",
      "   Analyse du profiler (CPU):\n",
      "     • Temps CPU total : 2.17 ms\n",
      "\n",
      "   Top 5 des opérations (CPU):\n",
      "----------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                  Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n",
      "----------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "       model_inference        35.42%     770.077us       100.00%       2.174ms       2.174ms          64 B     -78.06 KB             1  \n",
      "          aten::linear         3.36%      72.964us        46.46%       1.010ms      25.247us      39.06 KB           0 B            40  \n",
      "           aten::addmm        25.68%     558.190us        35.73%     776.759us      19.419us      39.06 KB      39.06 KB            40  \n",
      "           aten::slice         6.11%     132.861us         7.56%     164.261us       4.107us           0 B           0 B            40  \n",
      "               aten::t         2.93%      63.699us         7.37%     160.168us       4.004us           0 B           0 B            40  \n",
      "----------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 2.174ms\n",
      "\n",
      "\n",
      "   Trace exportée : trace_lr_cpu_run1.json\n",
      "     → Visualiser dans Chrome : chrome://tracing\n",
      "\n",
      "──────────────────────────────────────────────────────────────────────\n",
      " RUN #2 - Device : CPU (Batch size: 256)\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "   Démarrage du profilage...\n",
      "\n",
      "   Temps d'exécution : 0.0070 secondes\n",
      "\n",
      "   Analyse du profiler (CPU):\n",
      "     • Temps CPU total : 2.04 ms\n",
      "\n",
      "──────────────────────────────────────────────────────────────────────\n",
      " RUN #3 - Device : CPU (Batch size: 256)\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "   Démarrage du profilage...\n",
      "\n",
      "   Temps d'exécution : 0.0072 secondes\n",
      "\n",
      "   Analyse du profiler (CPU):\n",
      "     • Temps CPU total : 2.22 ms\n",
      "\n",
      " Statistiques CPU (3 runs):\n",
      "   • Temps médian : 0.0072 s\n",
      "   • Temps moyen : 1.0261 s\n",
      "   • Écart-type : 1.7649 s\n",
      "   • Tous les temps : ['3.0640s', '0.0070s', '0.0072s']\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"  PHASE 1 : BENCHMARK CPU\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "X_cpu = torch.from_numpy(X_np)\n",
    "\n",
    "cpu_times = []\n",
    "for i in range(NUM_RUNS):\n",
    "    cpu_time = run_benchmark('cpu', BATCH_SIZE_CPU, X_cpu, run_number=i+1)\n",
    "    cpu_times.append(cpu_time)\n",
    "    if i < NUM_RUNS - 1:\n",
    "        time.sleep(1)\n",
    "\n",
    "cpu_time_median = statistics.median(cpu_times)\n",
    "cpu_time_mean = statistics.mean(cpu_times)\n",
    "cpu_time_stdev = statistics.stdev(cpu_times) if len(cpu_times) > 1 else 0\n",
    "\n",
    "print(f\"\\n Statistiques CPU ({NUM_RUNS} runs):\")\n",
    "print(f\"   • Temps médian : {cpu_time_median:.4f} s\")\n",
    "print(f\"   • Temps moyen : {cpu_time_mean:.4f} s\")\n",
    "print(f\"   • Écart-type : {cpu_time_stdev:.4f} s\")\n",
    "print(f\"   • Tous les temps : {[f'{t:.4f}s' for t in cpu_times]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 Exécution du Benchmark GPU\n",
    "\n",
    "Sur GPU : transfert CPU→GPU des données + inférence. L'overhead peut rendre le GPU plus lent que le CPU pour un modèle aussi simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Aucun GPU détecté - Impossible de continuer le benchmark GPU\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\" PHASE 2 : BENCHMARK GPU\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\" GPU détecté : {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\" Mémoire GPU disponible : {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "\n",
    "    X_gpu = torch.from_numpy(X_np).cuda()\n",
    "\n",
    "    gpu_times = []\n",
    "    for i in range(NUM_RUNS):\n",
    "        if i > 0:\n",
    "            torch.cuda.empty_cache()\n",
    "            time.sleep(1)\n",
    "        gpu_time = run_benchmark('cuda', BATCH_SIZE_GPU, X_gpu, run_number=i+1)\n",
    "        gpu_times.append(gpu_time)\n",
    "\n",
    "    gpu_time_median = statistics.median(gpu_times)\n",
    "    gpu_time_mean = statistics.mean(gpu_times)\n",
    "    gpu_time_stdev = statistics.stdev(gpu_times) if len(gpu_times) > 1 else 0\n",
    "\n",
    "    print(f\"\\n Statistiques GPU ({NUM_RUNS} runs):\")\n",
    "    print(f\"   • Temps médian : {gpu_time_median:.4f} s\")\n",
    "    print(f\"   • Temps moyen : {gpu_time_mean:.4f} s\")\n",
    "    print(f\"   • Écart-type : {gpu_time_stdev:.4f} s\")\n",
    "    print(f\"   • Tous les temps : {[f'{t:.4f}s' for t in gpu_times]}\")\n",
    "else:\n",
    "    print(\"\\n Aucun GPU détecté - Impossible de continuer le benchmark GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9 Calcul du Speed Up et Analyse\n",
    "\n",
    "### Formule du Speed Up :\n",
    "\n",
    "$$\\text{Speed Up} = \\frac{T_{\\text{CPU}}}{T_{\\text{GPU}}}$$\n",
    "\n",
    "**Interprétation :**\n",
    "- Speed Up > 1 : GPU plus rapide\n",
    "- Speed Up = 1 : Performances égales\n",
    "- **Speed Up < 1 : CPU plus rapide** → cas typique pour la régression logistique !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Impossible de calculer le Speed Up sans GPU\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\" RÉSULTATS FINAUX\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    speed_up = cpu_time_median / gpu_time_median\n",
    "\n",
    "    print(f\"\\n  Temps d'exécution (médiane):\")\n",
    "    print(f\"   • CPU : {cpu_time_median:.4f} s\")\n",
    "    print(f\"   • GPU : {gpu_time_median:.4f} s\")\n",
    "    print(f\"\\n SPEED UP : {speed_up:.2f}x\")\n",
    "\n",
    "    time_saved = abs(cpu_time_median - gpu_time_median)\n",
    "    percent_faster = abs(cpu_time_median - gpu_time_median) / max(cpu_time_median, gpu_time_median) * 100\n",
    "\n",
    "    print(f\"\\n  Comparaison:\")\n",
    "    print(f\"   • Écart : {time_saved:.4f} s\")\n",
    "    if speed_up >= 1:\n",
    "        print(f\"   • GPU {percent_faster:.1f}% plus rapide\")\n",
    "    else:\n",
    "        print(f\"   • CPU {percent_faster:.1f}% plus rapide\")\n",
    "\n",
    "    throughput_cpu = NUM_SAMPLES / cpu_time_median\n",
    "    throughput_gpu = NUM_SAMPLES / gpu_time_median\n",
    "    print(f\"\\n  Throughput (échantillons/seconde):\")\n",
    "    print(f\"   • CPU : {throughput_cpu:.0f} samples/s\")\n",
    "    print(f\"   • GPU : {throughput_gpu:.0f} samples/s\")\n",
    "\n",
    "    print(f\"\\n{'─' * 70}\")\n",
    "    if speed_up < 1:\n",
    "        print(\" CONCLUSION : CPU plus rapide que le GPU !\")\n",
    "        print(f\"   Pour la régression logistique, le CPU est {1/speed_up:.1f}x plus rapide.\")\n",
    "        print(\"   L'overhead de transfert CPU→GPU et le faible parallélisme du modèle\")\n",
    "        print(\"   ne justifient pas l'utilisation du GPU. GPUs ≠ toujours mieux.\")\n",
    "    elif speed_up < 1.5:\n",
    "        print(\" CONCLUSION : Gain GPU marginal.\")\n",
    "        print(\"   Pour ce modèle simple, le GPU n'apporte qu'un gain limité.\")\n",
    "    else:\n",
    "        print(\" CONCLUSION : GPU plus rapide.\")\n",
    "        print(f\"   Le GPU est {speed_up:.1f}x plus rapide (cas moins fréquent pour la LR).\")\n",
    "    print(\"=\"*70)\n",
    "else:\n",
    "    print(\"\\n  Impossible de calculer le Speed Up sans GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10 Visualisation des Résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Aucun GPU - pas de visualisation comparative.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    devices = ['CPU', 'GPU']\n",
    "    times = [cpu_time_median, gpu_time_median]\n",
    "    colors = ['#FF6B6B', '#4ECDC4']\n",
    "\n",
    "    bars = axes[0].bar(devices, times, color=colors, alpha=0.7, edgecolor='black')\n",
    "    axes[0].set_ylabel('Temps (secondes)', fontsize=12)\n",
    "    axes[0].set_title('Comparaison CPU vs GPU (Régression Logistique)', fontsize=14, fontweight='bold')\n",
    "    axes[0].grid(axis='y', alpha=0.3)\n",
    "    for bar, t in zip(bars, times):\n",
    "        axes[0].text(bar.get_x() + bar.get_width()/2., bar.get_height(),\n",
    "                    f'{t:.4f}s', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "    throughputs = [throughput_cpu, throughput_gpu]\n",
    "    bars2 = axes[1].bar(devices, throughputs, color=colors, alpha=0.7, edgecolor='black')\n",
    "    axes[1].set_ylabel('Échantillons par seconde', fontsize=12)\n",
    "    axes[1].set_title('Throughput (samples/s)', fontsize=14, fontweight='bold')\n",
    "    axes[1].grid(axis='y', alpha=0.3)\n",
    "    for bar, tp in zip(bars2, throughputs):\n",
    "        axes[1].text(bar.get_x() + bar.get_width()/2., bar.get_height(),\n",
    "                    f'{tp:.0f}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('benchmark_logistic_regression.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"\\n Graphiques sauvegardés : benchmark_logistic_regression.png\")\n",
    "else:\n",
    "    print(\" Aucun GPU - pas de visualisation comparative.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Résumé et Notes Techniques\n",
    "\n",
    "### Points clés à retenir :\n",
    "\n",
    "1. **GPU ≠ toujours mieux** : Pour des modèles simples (régression logistique, petits réseaux), le CPU peut être plus rapide.\n",
    "2. **Overhead GPU** : Transfert des données CPU→GPU et lancement des kernels CUDA ont un coût fixe.\n",
    "3. **Quand utiliser le GPU** : Modèles profonds, gros batchs, beaucoup de calculs parallélisables (ex. BERT, ResNet).\n",
    "4. **Quand rester sur CPU** : Modèles linéaires, petits datasets, faible complexité.\n",
    "\n",
    "### Facteurs qui favorisent le CPU sur la régression logistique :\n",
    "\n",
    "- Peu de paramètres → peu d'opérations à paralléliser\n",
    "- Données déjà en RAM → pas de transfert\n",
    "- Bibliothèques CPU (e.g. scikit-learn, numpy) très optimisées pour ce cas\n",
    "\n",
    "### Pour aller plus loin :\n",
    "\n",
    "- Tester avec des datasets plus grands (100k+ samples) pour voir si le GPU finit par gagner\n",
    "- Comparer avec scikit-learn `LogisticRegression` (CPU uniquement, souvent très rapide)\n",
    "- Visualiser les traces dans Chrome : `chrome://tracing`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Références\n",
    "\n",
    "- [PyTorch Profiler Documentation](https://pytorch.org/docs/stable/profiler.html)\n",
    "- [When to use GPU vs CPU for ML](https://developer.nvidia.com/blog/when-to-use-gpu-acceleration/)\n",
    "- [CUDA Best Practices](https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/)\n",
    "\n",
    "---\n",
    "\n",
    "**Notebook :** P3 - Régression Logistique (GPU pas toujours mieux que CPU)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
