{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d499bac0",
   "metadata": {},
   "source": [
    "# GENERAL EXPLORATION OF MEMORY PINNING, NON BLOCKING SCHEDULING, and PARALLELISM ON ACCELERATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccb81640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.3)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n"
     ]
    }
   ],
   "source": [
    "%pip install torch "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a9b78f",
   "metadata": {},
   "source": [
    "import libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "497a3476",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "\n",
    "import torch \n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcf4966",
   "metadata": {},
   "source": [
    "check whether the GPU is available: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b518b249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "print(\"GPU is available\" if torch.cuda.is_available() else \"GPU not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92ef81b",
   "metadata": {},
   "source": [
    "##### we only use synthetic data to isolate and measure the impact of host/device transfer (pinned memory and non blocking concepts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c4f602",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SyntheticData(Dataset):\n",
    "    def __init__(self, size=1000000, feature_dim=1024):\n",
    "        self.data = torch.randn(size, feature_dim)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfccfb7",
   "metadata": {},
   "source": [
    "now we define a function to mesure the HOST/DEVICE transfer time: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0529046a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_iteration(device, dataloader, non_blocking=False):\n",
    "    transfer_times = []\n",
    "    total_times = []\n",
    "\n",
    "    for batch in dataloader:\n",
    "        start_total = time.perf_counter()\n",
    "\n",
    "        # Measure transfer\n",
    "        torch.cuda.synchronize()\n",
    "        start_transfer = time.perf_counter()\n",
    "        batch = batch.to(device, non_blocking=non_blocking)\n",
    "        torch.cuda.synchronize()\n",
    "        end_transfer = time.perf_counter()\n",
    "\n",
    "        #measure total time (to see that Non Blocking and number of workers has no effect here)\n",
    "        end_total = time.perf_counter()\n",
    "\n",
    "        transfer_times.append(end_transfer - start_transfer)\n",
    "        total_times.append(end_total - start_total)\n",
    "\n",
    "    return {\n",
    "        \"avg_transfer_time\": sum(transfer_times) / len(transfer_times),\n",
    "        \"avg_total_time\": sum(total_times) / len(total_times),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce184765",
   "metadata": {},
   "source": [
    "#### we test several combinations of number of workers, whether or not to pin memory, and wether or not enable non blocking behavior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a258fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SyntheticData()\n",
    "batch_size = 256\n",
    "\n",
    "configs = [\n",
    "    #no parallelism\n",
    "    {\"num_workers\": 0, \"pin_memory\": False, \"non_blocking\": False},\n",
    "    {\"num_workers\": 0, \"pin_memory\": False, \"non_blocking\": True},\n",
    "    {\"num_workers\": 0 , \"pin_memory\": True,  \"non_blocking\": False},\n",
    "    {\"num_workers\": 0, \"pin_memory\": True,  \"non_blocking\": True},\n",
    "\n",
    "    #num of workers suggested by torch in this device\n",
    "    {\"num_workers\": 2, \"pin_memory\": False, \"non_blocking\": False},\n",
    "    {\"num_workers\": 2, \"pin_memory\": False, \"non_blocking\": True},\n",
    "    {\"num_workers\": 2 , \"pin_memory\": True,  \"non_blocking\": False},\n",
    "    {\"num_workers\": 2, \"pin_memory\": True,  \"non_blocking\": True},\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ac38e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for cfg in configs:\n",
    "        loader = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            num_workers=cfg[\"num_workers\"],\n",
    "            pin_memory=cfg[\"pin_memory\"],\n",
    "        )\n",
    "        metrics = measure_iteration(torch.device(\"cuda\"), loader, non_blocking=cfg[\"non_blocking\"])\n",
    "        results.append((cfg, metrics))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6259e0",
   "metadata": {},
   "source": [
    "### We obtain the following results: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c35b1f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration: {'num_workers': 0, 'pin_memory': False, 'non_blocking': False}\n",
      "  avg_transfer_time: 0.36 micro seconds\n",
      "  avg_total_time: 0.45 micro seconds\n",
      "----------------------------------------\n",
      "Configuration: {'num_workers': 0, 'pin_memory': False, 'non_blocking': True}\n",
      "  avg_transfer_time: 0.33 micro seconds\n",
      "  avg_total_time: 0.36 micro seconds\n",
      "----------------------------------------\n",
      "Configuration: {'num_workers': 0, 'pin_memory': True, 'non_blocking': False}\n",
      "  avg_transfer_time: 0.14 micro seconds\n",
      "  avg_total_time: 0.16 micro seconds\n",
      "----------------------------------------\n",
      "Configuration: {'num_workers': 0, 'pin_memory': True, 'non_blocking': True}\n",
      "  avg_transfer_time: 0.12 micro seconds\n",
      "  avg_total_time: 0.15 micro seconds\n",
      "----------------------------------------\n",
      "Configuration: {'num_workers': 2, 'pin_memory': False, 'non_blocking': False}\n",
      "  avg_transfer_time: 0.74 micro seconds\n",
      "  avg_total_time: 0.79 micro seconds\n",
      "----------------------------------------\n",
      "Configuration: {'num_workers': 2, 'pin_memory': False, 'non_blocking': True}\n",
      "  avg_transfer_time: 0.64 micro seconds\n",
      "  avg_total_time: 0.68 micro seconds\n",
      "----------------------------------------\n",
      "Configuration: {'num_workers': 2, 'pin_memory': True, 'non_blocking': False}\n",
      "  avg_transfer_time: 0.23 micro seconds\n",
      "  avg_total_time: 0.3 micro seconds\n",
      "----------------------------------------\n",
      "Configuration: {'num_workers': 2, 'pin_memory': True, 'non_blocking': True}\n",
      "  avg_transfer_time: 0.19 micro seconds\n",
      "  avg_total_time: 0.26 micro seconds\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for cfg, metrics in results:\n",
    "    print(\"Configuration:\", cfg)\n",
    "    for k, v in metrics.items():\n",
    "        print(f\"  {k}: {round(v*1e3, 2)} micro seconds\")\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7b99fe",
   "metadata": {},
   "source": [
    "#### as we can see, the most acceleration effect in this case comes from **memory pinning** \n",
    "as it's directly related to HOST2DEVICE transfer of data, so total time is the same as transfer time. \n",
    "memory pinning means pinning the necessary data physical adresses in memory (no paging), so OS cannot move it nor swap it to disk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07dc8fa3",
   "metadata": {},
   "source": [
    "**NOTE:** we can see that Non Blocking has no significant effect in this case as we don't have any GPU computations. the same goes for the Number of Workers (which in fact harmed acceleration) as we're not doing any preprocessing or loading of data from disk (because we're using synthetic data that's already in memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c2fd71",
   "metadata": {},
   "source": [
    "#### in the following cell, we will add some GPU Computation to remarque the effect of Non Blocking argument: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "daceb8fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration: {'num_workers': 0, 'pin_memory': False, 'non_blocking': False}\n",
      "  avg_transfer_time: 0.36 micro seconds\n",
      "  avg_total_time: 0.45 micro seconds\n",
      "----------------------------------------\n",
      "Configuration: {'num_workers': 0, 'pin_memory': False, 'non_blocking': True}\n",
      "  avg_transfer_time: 0.33 micro seconds\n",
      "  avg_total_time: 0.36 micro seconds\n",
      "----------------------------------------\n",
      "Configuration: {'num_workers': 0, 'pin_memory': True, 'non_blocking': False}\n",
      "  avg_transfer_time: 0.14 micro seconds\n",
      "  avg_total_time: 0.16 micro seconds\n",
      "----------------------------------------\n",
      "Configuration: {'num_workers': 0, 'pin_memory': True, 'non_blocking': True}\n",
      "  avg_transfer_time: 0.12 micro seconds\n",
      "  avg_total_time: 0.15 micro seconds\n",
      "----------------------------------------\n",
      "Configuration: {'num_workers': 2, 'pin_memory': False, 'non_blocking': False}\n",
      "  avg_transfer_time: 0.74 micro seconds\n",
      "  avg_total_time: 0.79 micro seconds\n",
      "----------------------------------------\n",
      "Configuration: {'num_workers': 2, 'pin_memory': False, 'non_blocking': True}\n",
      "  avg_transfer_time: 0.64 micro seconds\n",
      "  avg_total_time: 0.68 micro seconds\n",
      "----------------------------------------\n",
      "Configuration: {'num_workers': 2, 'pin_memory': True, 'non_blocking': False}\n",
      "  avg_transfer_time: 0.23 micro seconds\n",
      "  avg_total_time: 0.3 micro seconds\n",
      "----------------------------------------\n",
      "Configuration: {'num_workers': 2, 'pin_memory': True, 'non_blocking': True}\n",
      "  avg_transfer_time: 0.19 micro seconds\n",
      "  avg_total_time: 0.26 micro seconds\n",
      "----------------------------------------\n",
      "Configuration: {'num_workers': 0, 'pin_memory': False, 'non_blocking': False}\n",
      "  avg_GPU_time: 0.23 micro seconds\n",
      "  avg_total_time: 0.61 micro seconds\n",
      "----------------------------------------\n",
      "Configuration: {'num_workers': 0, 'pin_memory': False, 'non_blocking': True}\n",
      "  avg_GPU_time: 0.16 micro seconds\n",
      "  avg_total_time: 0.5 micro seconds\n",
      "----------------------------------------\n",
      "Configuration: {'num_workers': 0, 'pin_memory': True, 'non_blocking': False}\n",
      "  avg_GPU_time: 0.16 micro seconds\n",
      "  avg_total_time: 0.32 micro seconds\n",
      "----------------------------------------\n",
      "Configuration: {'num_workers': 0, 'pin_memory': True, 'non_blocking': True}\n",
      "  avg_GPU_time: 0.16 micro seconds\n",
      "  avg_total_time: 0.28 micro seconds\n",
      "----------------------------------------\n",
      "Configuration: {'num_workers': 2, 'pin_memory': False, 'non_blocking': False}\n",
      "  avg_GPU_time: 0.21 micro seconds\n",
      "  avg_total_time: 0.95 micro seconds\n",
      "----------------------------------------\n",
      "Configuration: {'num_workers': 2, 'pin_memory': False, 'non_blocking': True}\n",
      "  avg_GPU_time: 0.22 micro seconds\n",
      "  avg_total_time: 0.86 micro seconds\n",
      "----------------------------------------\n",
      "Configuration: {'num_workers': 2, 'pin_memory': True, 'non_blocking': False}\n",
      "  avg_GPU_time: 0.23 micro seconds\n",
      "  avg_total_time: 0.48 micro seconds\n",
      "----------------------------------------\n",
      "Configuration: {'num_workers': 2, 'pin_memory': True, 'non_blocking': True}\n",
      "  avg_GPU_time: 0.24 micro seconds\n",
      "  avg_total_time: 0.44 micro seconds\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def measure_iteration(device, dataloader, non_blocking=False):\n",
    "    compute_times = []\n",
    "    total_times = []\n",
    "\n",
    "    for batch in dataloader:\n",
    "        start_total = time.perf_counter()\n",
    "\n",
    "        batch = batch.to(device, non_blocking=non_blocking)\n",
    "\n",
    "        #add some GPU Computation \n",
    "        torch.cuda.synchronize()\n",
    "        start_compute = time.perf_counter()\n",
    "        batch @ (batch.T)**8\n",
    "        torch.cuda.synchronize()\n",
    "        end_compute = time.perf_counter()\n",
    "\n",
    "        end_total = time.perf_counter()\n",
    "\n",
    "        compute_times.append(end_compute - start_compute)\n",
    "        total_times.append(end_total - start_total)\n",
    "\n",
    "    return {\n",
    "        \"avg_GPU_time\": sum(compute_times) / len(compute_times),\n",
    "        \"avg_total_time\": sum(total_times) / len(total_times),\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "configs = [\n",
    "    #no parallelism\n",
    "    {\"num_workers\": 0, \"pin_memory\": False, \"non_blocking\": False},\n",
    "    {\"num_workers\": 0, \"pin_memory\": False, \"non_blocking\": True},\n",
    "    {\"num_workers\": 0 , \"pin_memory\": True,  \"non_blocking\": False},\n",
    "    {\"num_workers\": 0, \"pin_memory\": True,  \"non_blocking\": True},\n",
    "\n",
    "    #num of workers suggested by torch in this device\n",
    "    {\"num_workers\": 2, \"pin_memory\": False, \"non_blocking\": False},\n",
    "    {\"num_workers\": 2, \"pin_memory\": False, \"non_blocking\": True},\n",
    "    {\"num_workers\": 2 , \"pin_memory\": True,  \"non_blocking\": False},\n",
    "    {\"num_workers\": 2, \"pin_memory\": True,  \"non_blocking\": True},\n",
    "]\n",
    "\n",
    "\n",
    "esults = []\n",
    "\n",
    "for cfg in configs:\n",
    "        loader = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            num_workers=cfg[\"num_workers\"],\n",
    "            pin_memory=cfg[\"pin_memory\"],\n",
    "        )\n",
    "        metrics = measure_iteration(torch.device(\"cuda\"), loader, non_blocking=cfg[\"non_blocking\"])\n",
    "        results.append((cfg, metrics))\n",
    "\n",
    "for cfg, metrics in results:\n",
    "    print(\"Configuration:\", cfg)\n",
    "    for k, v in metrics.items():\n",
    "        print(f\"  {k}: {round(v*1e3, 2)} micro seconds\")\n",
    "    print(\"-\" * 40)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae8f287",
   "metadata": {},
   "source": [
    "#### As we can see, Non Blocking only enhances acceleration in case of presence of GPU operations, \n",
    "**NOTE:** Non Blocking requires Memory Pinning in order to enhance acceleration in case of present GPU computations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3c3e06",
   "metadata": {},
   "source": [
    "it does so by allowing the CPU not to wait for the host/device transfer to finish before scheduling GPU operations, however, this requires no OS interventions during transfer and also fix data memory adresses, and these requirements are only satisfied when we use Memory Pinning. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93c63e5",
   "metadata": {},
   "source": [
    "Furthermore, with synthetic data, increasing num_workers does not improve performance, because there's no I/O bound task or preprocessing as data is already in memory.\n",
    "To showcase this, we will modify the SyntheticData class to introduce artificial CPU preprocessing and I/O latency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e26c0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SyntheticIODataset(Dataset):\n",
    "    def __init__(\n",
    "    self,\n",
    "    size=100_000,\n",
    "    feature_dim=1024,\n",
    "    io_delay=0.002, # simulate I/O bound task\n",
    "    cpu_work=True   #enable CPU preprocessing\n",
    "    ):\n",
    "        self.data = torch.randn(size, feature_dim)\n",
    "        self.io_delay = io_delay\n",
    "        self.cpu_work = cpu_work\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # simulate I/O-bound latency \n",
    "        if self.io_delay > 0:\n",
    "            time.sleep(self.io_delay)\n",
    "\n",
    "        x = self.data[idx]\n",
    "\n",
    "        #simulate CPU-bound preprocessing\n",
    "        if self.cpu_work:\n",
    "            #reshape: 1D to 2D \n",
    "            x = x.view(32, 32)\n",
    "\n",
    "            #typical CPU preprocessing ops\n",
    "            x = (x - x.mean()) / (x.std() + 1e-6)\n",
    "\n",
    "            #force memory access + compute\n",
    "            x = x * 1.1 + 0.1\n",
    "\n",
    "            #force a copy for more real simulation\n",
    "            x = x.contiguous().view(-1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edd8e8b",
   "metadata": {},
   "source": [
    "we will now define the function that will help us do the measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f19db517",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def benchmark_dataloader(\n",
    "    dataset,\n",
    "    batch_size=256,\n",
    "    num_workers=0,\n",
    "    pin_memory=False,\n",
    "    device=\"cpu\",\n",
    "    num_batches=100,\n",
    "    warmup_batches=10,\n",
    "):\n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    it = iter(loader)\n",
    "\n",
    "    for _ in range(warmup_batches):\n",
    "        batch = next(it)\n",
    "        if device == \"cuda\":\n",
    "            batch = batch.to(device, non_blocking=pin_memory)\n",
    "\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    start = time.perf_counter()\n",
    "\n",
    "    for _ in range(num_batches):\n",
    "        batch = next(it)\n",
    "        if device == \"cuda\":\n",
    "            batch = batch.to(device, non_blocking=pin_memory)\n",
    "\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    end = time.perf_counter()\n",
    "\n",
    "    total_time = end - start\n",
    "    samples = num_batches * batch_size\n",
    "\n",
    "    return {\n",
    "        \"num_workers\": num_workers,\n",
    "        \"pin_memory\": pin_memory,\n",
    "        \"device\": device,\n",
    "        \"avg_iter_time_ms\": (total_time / num_batches) * 1e3,\n",
    "        \"throughput_samples_per_sec\": samples / total_time,\n",
    "    }\n",
    "\n",
    "\n",
    "dataset = SyntheticIODataset(\n",
    "    io_delay=0.002,\n",
    "    cpu_work=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8fbc3c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_workers': 0, 'pin_memory': True, 'device': 'cuda', 'avg_iter_time_ms': 572.8698136900016, 'throughput_samples_per_sec': 446.87290878714356}\n",
      "{'num_workers': 2, 'pin_memory': True, 'device': 'cuda', 'avg_iter_time_ms': 291.3731124500009, 'throughput_samples_per_sec': 878.598570223013}\n",
      "{'num_workers': 4, 'pin_memory': True, 'device': 'cuda', 'avg_iter_time_ms': 146.57597732000113, 'throughput_samples_per_sec': 1746.5344913996855}\n",
      "{'num_workers': 8, 'pin_memory': True, 'device': 'cuda', 'avg_iter_time_ms': 70.49252318000072, 'throughput_samples_per_sec': 3631.590819161219}\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    dataset = SyntheticIODataset(\n",
    "        io_delay=0.002,\n",
    "        cpu_work=True\n",
    "    )\n",
    "\n",
    "    gpu_results = []\n",
    "\n",
    "    for nw in [0, 2, 4, 8]:\n",
    "        res = benchmark_dataloader(\n",
    "            dataset,\n",
    "            batch_size=256,\n",
    "            num_workers=nw,\n",
    "            pin_memory=True,\n",
    "            device=\"cuda\"\n",
    "        )\n",
    "        gpu_results.append(res)\n",
    "\n",
    "    for r in gpu_results:\n",
    "        print(r)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db0ff4c",
   "metadata": {},
   "source": [
    "### as we can see, Increasing the number of workers improves throughput by parallelizing data preparation and hiding latency.\n",
    " this confirms that num_workers optimizes the DATA LOADING AND PREPROCESSING stage rather than memory transfer or GPU computation, in contrast to before having any I/O or CPU preprocessing where adding parallelism increased latency."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
