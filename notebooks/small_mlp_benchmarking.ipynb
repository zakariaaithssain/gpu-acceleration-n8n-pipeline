{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d487172",
   "metadata": {},
   "source": [
    "# Benchmarking of CPU vs GPU for a MLP on a Small Dataset (not ready yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cecae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cu126)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.3)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n"
     ]
    }
   ],
   "source": [
    "%pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "cd723454",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision.io import decode_image, ImageReadMode\n",
    "\n",
    "from torchvision.models import resnet50, ResNet50_Weights \n",
    "\n",
    "import torch\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62700ae3",
   "metadata": {},
   "source": [
    "### check whether GPU is available or not: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "c4fcbc0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "print(\"GPU is available\" if torch.cuda.is_available() else \"GPU is not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14aa1779",
   "metadata": {},
   "source": [
    "### define the dataset class for our inference images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "cb320f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceDataset(Dataset):\n",
    "    def __init__(self, images_dir: str, transform = None): \n",
    "        self.images_dir = images_dir\n",
    "        self.transform = transform\n",
    "        self.images = sorted([img for img in os.listdir(self.images_dir) \n",
    "                              if img.lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
    "                              ])\n",
    "    \n",
    "\n",
    "    def __len__(self): \n",
    "        return len(self.images)\n",
    "    \n",
    "\n",
    "    \n",
    "    def __getitem__(self, idx): \n",
    "        img_path = os.path.join(self.images_dir, self.images[idx])\n",
    "        img = decode_image(img_path, mode = ImageReadMode.RGB)\n",
    "        \n",
    "        if self.transform: \n",
    "            img = self.transform(img)\n",
    "        \n",
    "        return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a8d902",
   "metadata": {},
   "source": [
    "### define the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "6d80ada1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(dataset, batch_size, num_workers, pin_memory):\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,        \n",
    "        shuffle=False, # no shuffling to keep the order of images processing identical for fair comparison\n",
    "        num_workers=num_workers,        \n",
    "        pin_memory=pin_memory   \n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc849a98",
   "metadata": {},
   "source": [
    "### define `gpu_inference` function for GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "bfe0916d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpu_inference(model, loader, non_blocking = True): \n",
    "    device = torch.device('cuda')\n",
    "    #move the model to gpu \n",
    "    model = model.to(device)\n",
    "    #set to evaluation mode, this disables dropouts and batch normalizations\n",
    "    model.eval()\n",
    "     \n",
    "    torch.cuda.synchronize()\n",
    "    start = time.perf_counter()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x in loader:\n",
    "            x = x.to(device, non_blocking=non_blocking)\n",
    "            _ = model(x)\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    end = time.perf_counter()\n",
    "\n",
    "    gpu_time = end - start\n",
    "    print(\"GPU time (s):\", gpu_time)\n",
    "    return gpu_time\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d685c8f",
   "metadata": {},
   "source": [
    "### define `cpu_inference` function for CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "c7d80ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cpu_inference(model, loader): \n",
    "    device = torch.device(\"cpu\")\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    with torch.no_grad():\n",
    "        for x in loader:\n",
    "            x.to(device)\n",
    "            _ = model(x)\n",
    "\n",
    "    end = time.perf_counter()\n",
    "\n",
    "    cpu_time = end - start\n",
    "    print(\"CPU time (s):\", cpu_time)\n",
    "    return cpu_time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30461eaf",
   "metadata": {},
   "source": [
    "### import the ResNet50 model with default (best availabe) weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "90dd25d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = ResNet50_Weights.DEFAULT\n",
    "model = resnet50(weights=weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b82b20e",
   "metadata": {},
   "source": [
    "### weights come with the preprocessing pipeline matching the characteristics of the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "0fbbf234",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = weights.transforms()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcf219f",
   "metadata": {},
   "source": [
    "### here comes the fun: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00c4d54",
   "metadata": {},
   "source": [
    "first we should clone our repo so we can load data to colab from VS code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "5d3be4a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'gpu-acceleration-n8n-pipeline' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "! git clone https://github.com/zakariaaithssain/gpu-acceleration-n8n-pipeline.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "6a01bc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_DIR = \"/content/gpu-acceleration-n8n-pipeline/notebooks/images\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff9e855",
   "metadata": {},
   "source": [
    "### we use all the acceleration methods we discussed in the `overall_exploration` notebook, and then compare the result to the CPU default behavior. \n",
    "**Note:** we set a high batch size because GPU excels more with them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "457bbec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = InferenceDataset(images_dir=SAMPLE_DIR, transform=preprocess)\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "#uses pinned memory and parallelism\n",
    "accelerated_loader = data_loader(dataset, batch_size=BATCH_SIZE, num_workers=2, pin_memory=True)\n",
    "#no pinned memory nor parallelism\n",
    "default_loader = data_loader(dataset, batch_size=BATCH_SIZE, num_workers=0, pin_memory=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12f1aa5",
   "metadata": {},
   "source": [
    "#### now we launch inference test for both devices: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "6acafc2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU time (s): 249.310788408\n"
     ]
    }
   ],
   "source": [
    "cpu_time = cpu_inference(model, default_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10eb40c6",
   "metadata": {},
   "source": [
    "#### we use the following formula to calculate the graphical acceleration: \n",
    "**SPEED UP = CPU TIME / GPU TIME**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "5ab49a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU time (s): 7.951139635999425\n",
      "SPEED UP:  31.355352794865446\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available(): \n",
    "    gpu_time = gpu_inference(model, accelerated_loader, non_blocking=True)\n",
    "    speed_up = cpu_time/gpu_time \n",
    "    print(\"SPEED UP: \", speed_up)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbf4156",
   "metadata": {},
   "source": [
    "### CONCLUSION:  \n",
    "As we can see, combining: \n",
    " - using GPU as the device\n",
    " - parallel data preprocessing in CPU (num_workers > 0 in DataLoader)\n",
    " - memory pinning (pin_memory is True in DataLoader)\n",
    " - non blocking CPU scheduling (non_blocking is True when transfering to device)\n",
    " - and a big enough batch size (100 samples per batch)\n",
    "\n",
    " makes inference more than **30 times faster** than using CPU as the device with no further configuration.  \n",
    "\n",
    " **Note:** this is not a fair enough comparison, as we didn't take in consideration GPU warmup before counting performance, if we did so, results would be much greater"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
